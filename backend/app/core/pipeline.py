"""Pipeline æµæ°´çº¿ - æ•´åˆé‡‡é›†ã€å¤„ç†ã€åˆ†æã€è¾“å‡º"""
from datetime import datetime, timedelta
from typing import List, Optional, Dict
from uuid import UUID
import asyncio
import yaml
from pathlib import Path

from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.models.database import async_session_maker, WatchlistItem
from app.models.schemas import (
    NewsItemCreate, RawItemCreate, AnalysisResultCreate,
    PipelineRunUpdate, DeliveryLogCreate, DeliveryLogUpdate
)
from app.models import crud
from app.collectors.base import RawNewsData
from app.collectors.finnhub import FinnhubNewsCollector
from app.collectors.sec_edgar import SECFilingCollector
from app.core.normalizer import DataProcessor
from app.providers.factory import get_ai_provider
from app.providers.base import AIAnalysisError
from app.outputs.base import Digest, DigestItem, TickerSummary
from app.outputs.notion import NotionOutput
from app.outputs.markdown import MarkdownOutput
from app.outputs.telegram import TelegramOutput
from app.outputs.email import EmailOutput
from app.utils.logger import get_logger, set_run_id, get_run_id

logger = get_logger(__name__)


class Pipeline:
    """
    æ–°é—»å¤„ç†æµæ°´çº¿
    
    æµç¨‹:
    1. åŠ è½½ Watchlist
    2. é‡‡é›†æ–°é—» (Finnhub, SEC)
    3. æ ‡å‡†åŒ– + å»é‡
    4. AI åˆ†æ
    5. ä¿å­˜åˆ°æ•°æ®åº“
    6. è¾“å‡ºåˆ° Notion
    """
    
    def __init__(
        self,
        hours_lookback: int = 24,
        tickers: Optional[List[str]] = None,
        limit_per_ticker: Optional[int] = None
    ):
        self.hours_lookback = hours_lookback
        self.specified_tickers = tickers
        self.limit_per_ticker = limit_per_ticker
        
        # ç»Ÿè®¡
        self.stats = {
            "raw_collected": 0,
            "after_normalize": 0,
            "after_dedup": 0,
            "analyzed_success": 0,
            "analyzed_failed": 0,
            "delivered": 0,
        }
    
    async def run(self, run_id: Optional[UUID] = None) -> Digest:
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            run_id: è¿è¡Œ IDï¼ˆç”¨äºè¿½è¸ªï¼‰
        
        Returns:
            ç”Ÿæˆçš„æ‘˜è¦
        """
        # è®¾ç½® run_id
        if run_id:
            set_run_id(run_id)
        else:
            run_id = get_run_id() or set_run_id()
        
        logger.info("Pipeline started", run_id=str(run_id))
        
        try:
            # Step 1: åŠ è½½ Watchlist
            watchlist = await self._load_watchlist()
            tickers = self.specified_tickers or [item["ticker"] for item in watchlist]
            thesis_map = {item["ticker"]: item.get("thesis", "") for item in watchlist}
            
            logger.info(f"Watchlist loaded: {len(tickers)} tickers")
            
            # Step 2: è®¡ç®—æ—¶é—´çª—å£
            window_end = datetime.utcnow()
            window_start = window_end - timedelta(hours=self.hours_lookback)
            
            # Step 3: é‡‡é›†æ–°é—»
            raw_items = await self._collect_news(tickers, window_start, window_end)
            self.stats["raw_collected"] = len(raw_items)
            
            logger.info(f"Collected {len(raw_items)} raw items")
            
            # Step 4: æ ‡å‡†åŒ– + å»é‡
            processor = DataProcessor()
            normalized_items, total_before, removed = processor.process(raw_items)
            self.stats["after_normalize"] = len(normalized_items)
            self.stats["after_dedup"] = len(normalized_items)
            
            logger.info(f"After processing: {len(normalized_items)} items (removed {removed})")
            
            # Step 4.5: å¦‚æœè®¾ç½®äº†é™åˆ¶ï¼Œæ¯åªè‚¡ç¥¨åªä¿ç•™ N æ¡æ–°é—»
            if self.limit_per_ticker:
                limited_items = []
                ticker_counts: Dict[str, int] = {}
                
                for item in normalized_items:
                    raw_create, news_create = item
                    # æ£€æŸ¥è¯¥æ–°é—»çš„æ‰€æœ‰è‚¡ç¥¨æ˜¯å¦å·²è¾¾ä¸Šé™
                    should_include = False
                    if news_create.tickers:
                        for ticker in news_create.tickers:
                            count = ticker_counts.get(ticker, 0)
                            if count < self.limit_per_ticker:
                                should_include = True
                                break
                    else:
                        should_include = True
                    
                    if should_include:
                        limited_items.append(item)
                        if news_create.tickers:
                            for ticker in news_create.tickers:
                                ticker_counts[ticker] = ticker_counts.get(ticker, 0) + 1
                
                original_count = len(normalized_items)
                normalized_items = limited_items
                logger.info(f"Limited to {len(normalized_items)} items (limit: {self.limit_per_ticker}/ticker, was: {original_count})")
            
            # Step 5: ä¿å­˜åˆ°æ•°æ®åº“å¹¶è¿›è¡Œ AI åˆ†æ
            digest_items = await self._analyze_and_save(normalized_items, thesis_map)
            
            # Step 6: ç”Ÿæˆæ¯åªè‚¡ç¥¨çš„æ±‡æ€»åˆ†æ
            ticker_summaries = await self._generate_ticker_summaries(
                digest_items, watchlist, thesis_map
            )
            
            # Step 7: åˆ›å»º Digest
            digest = Digest(
                run_id=str(run_id),
                generated_at=datetime.utcnow(),
                window_start=window_start,
                window_end=window_end,
                items=digest_items,
                total_collected=self.stats["raw_collected"],
                total_after_dedup=self.stats["after_dedup"],
                total_analyzed=self.stats["analyzed_success"],
                total_failed=self.stats["analyzed_failed"],
                ticker_summaries=ticker_summaries,
            )
            
            # Step 7: è¾“å‡º
            logger.info(f"Configured outputs: {settings.outputs}")
            
            if "notion" in settings.outputs:
                logger.info("Delivering to Notion...")
                await self._deliver_to_notion(digest, run_id)
            else:
                logger.debug("Notion output not enabled")
            
            if "markdown" in settings.outputs:
                logger.info("Delivering to Markdown...")
                await self._deliver_to_markdown(digest, run_id)
            else:
                logger.debug("Markdown output not enabled")
            
            if "telegram" in settings.outputs:
                logger.info("Delivering to Telegram...")
                await self._deliver_to_telegram(digest, run_id)
            else:
                logger.debug("Telegram output not enabled (not in OUTPUTS or not configured)")
            
            if "email" in settings.outputs:
                logger.info("Delivering to Email...")
                await self._deliver_to_email(digest, run_id)
            else:
                logger.debug("Email output not enabled (not in OUTPUTS or not configured)")
            
            # æ›´æ–° Pipeline Run çŠ¶æ€
            await self._update_pipeline_run(run_id, "success")
            
            logger.info(
                "Pipeline completed",
                run_id=str(run_id),
                stats=self.stats
            )
            
            return digest
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}", run_id=str(run_id))
            await self._update_pipeline_run(run_id, "failed", str(e))
            raise
    
    async def _load_watchlist(self) -> List[Dict]:
        """ä» YAML æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½ Watchlist"""
        # ä¼˜å…ˆä» YAML åŠ è½½
        yaml_path = Path(settings.watchlist_path)
        if yaml_path.exists():
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                return data.get("watchlist", [])
        
        # ä»æ•°æ®åº“åŠ è½½
        async with async_session_maker() as db:
            items = await crud.get_watchlist(db)
            return [
                {
                    "ticker": item.ticker,
                    "company_name": item.company_name,
                    "thesis": item.thesis,
                    "risk_tags": item.risk_tags,
                    "priority": item.priority,
                    "sector": item.sector,
                }
                for item in items
            ]
    
    async def _collect_news(
        self,
        tickers: List[str],
        since: datetime,
        until: datetime
    ) -> List[RawNewsData]:
        """ä»æ‰€æœ‰æ•°æ®æºé‡‡é›†æ–°é—»"""
        all_items: List[RawNewsData] = []
        
        # Finnhub (ä¸­å¯ä¿¡åº¦æ–°é—»)
        if settings.finnhub_enabled:
            try:
                async with FinnhubNewsCollector() as collector:
                    items = await collector.collect(tickers, since, until)
                    all_items.extend(items)
                    logger.info(f"Finnhub collected {len(items)} items")
            except Exception as e:
                logger.error(f"Finnhub collection failed: {e}")
        
        # SEC EDGAR (é«˜å¯ä¿¡åº¦å…¬å‘Š)
        if settings.sec_enabled:
            try:
                async with SECFilingCollector() as collector:
                    items = await collector.collect(tickers, since, until)
                    all_items.extend(items)
                    logger.info(f"SEC collected {len(items)} filings")
            except Exception as e:
                logger.error(f"SEC collection failed: {e}")
        
        return all_items
    
    async def _analyze_and_save(
        self,
        normalized_items: List[tuple],
        thesis_map: Dict[str, str]
    ) -> List[DigestItem]:
        """AI åˆ†æå¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        digest_items: List[DigestItem] = []
        
        # è·å– AI Provider
        logger.info("=" * 50)
        logger.info("ğŸ¤– STEP: Creating AI Provider")
        logger.info("=" * 50)
        try:
            from app.config import settings as cfg
            logger.info(f"Config - ai_provider: {cfg.ai_provider}")
            logger.info(f"Config - gemini_api_key set: {bool(cfg.gemini_api_key)} (len={len(cfg.gemini_api_key) if cfg.gemini_api_key else 0})")
            logger.info(f"Config - gemini_model: {cfg.gemini_model}")
            
            provider = get_ai_provider()
            logger.info(f"âœ… AI provider created: {provider.provider_name} / {provider.model_name}")
        except Exception as e:
            import traceback
            logger.error(f"âŒ Failed to create AI provider: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            logger.warning("âš ï¸ Continuing without AI analysis - all news will be marked as neutral")
            logger.info("=" * 50)
            logger.info("ğŸ“ STEP: Saving News WITHOUT AI Analysis")
            logger.info("=" * 50)
            # æ²¡æœ‰ AIï¼Œä»ç„¶ä¿å­˜æ–°é—»ä½†ä¸åˆ†æ
            async with async_session_maker() as db:
                for raw_create, news_create in normalized_items:
                    # ä¿å­˜åŸå§‹æ•°æ®
                    raw_item = await crud.create_raw_item(db, raw_create)
                    news_create.raw_item_id = raw_item.id
                    
                    # ä¿å­˜æ–°é—»
                    await crud.create_news_item(db, news_create)
                    
                    digest_items.append(DigestItem(news=news_create, analysis=None))
                
                await db.commit()
            
            return digest_items
        
        # æœ‰ AIï¼Œè¿›è¡Œåˆ†æ
        logger.info("=" * 50)
        logger.info("ğŸš€ STEP: Starting AI Analysis (WITH AI)")
        logger.info("=" * 50)
        logger.info(f"Provider: {provider.provider_name} / {provider.model_name}")
        logger.info(f"Items to analyze: {len(normalized_items)}")
        analyzed_count = 0
        skipped_count = 0
        
        try:
            async with provider:
                async with async_session_maker() as db:
                    for i, (raw_create, news_create) in enumerate(normalized_items):
                        logger.info(f"[{i+1}/{len(normalized_items)}] Processing: {news_create.title[:50]}")
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆURL æˆ– Hash å»é‡ï¼‰
                        existing = await crud.get_news_item_by_url(db, news_create.canonical_url)
                        if existing:
                            logger.info(f"[{i+1}] â­ï¸ Skipping duplicate (URL already in DB)")
                            skipped_count += 1
                            continue
                        
                        logger.info(f"[{i+1}] ğŸ” Calling AI for analysis...")
                        
                        # ä¿å­˜åŸå§‹æ•°æ®
                        raw_item = await crud.create_raw_item(db, raw_create)
                        news_create.raw_item_id = raw_item.id
                        
                        # ä¿å­˜æ–°é—»
                        news_item = await crud.create_news_item(db, news_create)
                        
                        # AI åˆ†æ
                        analysis = None
                        try:
                            # è·å–ç›¸å…³ thesis
                            thesis = ""
                            if news_create.tickers:
                                for ticker in news_create.tickers:
                                    if ticker in thesis_map:
                                        thesis = thesis_map[ticker]
                                        break
                            
                            analysis_output, tokens, cost = await provider.analyze(
                                news_create, thesis
                            )
                            
                            # ä¿å­˜åˆ†æç»“æœ
                            analysis_create = AnalysisResultCreate(
                                news_item_id=news_item.id,
                                provider=provider.provider_name,
                                model=provider.model_name,
                                prompt_version=provider.prompt_version,
                                event_type=analysis_output.event_type,
                                impact_direction=analysis_output.impact_direction,
                                impact_horizon=analysis_output.impact_horizon,
                                thesis_relation=analysis_output.thesis_relation,
                                confidence=analysis_output.confidence,
                                confidence_reason=analysis_output.confidence_reason,
                                summary=analysis_output.summary,
                                key_facts=analysis_output.key_facts,
                                watch_next=analysis_output.watch_next,
                                tokens_used=tokens,
                                cost_usd=cost,
                            )
                            await crud.create_analysis_result(db, analysis_create)
                            
                            analysis = analysis_output
                            self.stats["analyzed_success"] += 1
                            analyzed_count += 1
                            logger.info(f"[{i+1}] âœ… Analysis success: {analysis_output.impact_direction} ({analysis_output.event_type})")
                            
                        except Exception as e:
                            import traceback
                            logger.warning(f"[{i+1}] âŒ Analysis failed: {type(e).__name__}: {e}")
                            logger.debug(f"Traceback: {traceback.format_exc()}")
                            self.stats["analyzed_failed"] += 1
                        
                        digest_items.append(DigestItem(news=news_create, analysis=analysis))
                    
                    await db.commit()
                    
            logger.info(f"ğŸ“Š Analysis complete: {analyzed_count} success, {self.stats['analyzed_failed']} failed, {skipped_count} skipped")
            
        except Exception as e:
            import traceback
            logger.error(f"Error during AI analysis: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
        
        return digest_items
    
    async def _deliver_to_notion(self, digest: Digest, run_id: UUID):
        """è¾“å‡ºåˆ° Notion"""
        try:
            async with async_session_maker() as db:
                # åˆ›å»º delivery log
                log = await crud.create_delivery_log(db, DeliveryLogCreate(
                    run_id=str(run_id),
                    channel="notion",
                    status="pending"
                ))
                await db.commit()
                
                # å‘é€
                async with NotionOutput() as output:
                    page_id = await output.deliver(digest)
                
                # æ›´æ–°çŠ¶æ€
                await crud.update_delivery_log(db, log.id, DeliveryLogUpdate(
                    status="success",
                    notion_page_id=page_id
                ))
                await db.commit()
                
                self.stats["delivered"] += 1
                logger.info(f"Delivered to Notion: {page_id}")
                
        except Exception as e:
            logger.error(f"Notion delivery failed: {e}")
            async with async_session_maker() as db:
                # å°è¯•æ›´æ–°å¤±è´¥çŠ¶æ€
                try:
                    await crud.update_delivery_log(db, log.id, DeliveryLogUpdate(
                        status="failed",
                        error_message=str(e)
                    ))
                    await db.commit()
                except:
                    pass
    
    async def _deliver_to_markdown(self, digest: Digest, run_id: UUID):
        """è¾“å‡ºåˆ° Markdown æ–‡ä»¶"""
        try:
            async with MarkdownOutput() as output:
                filepath = await output.deliver(digest)
            
            self.stats["delivered"] += 1
            logger.info(f"Delivered to Markdown: {filepath}")
            
        except Exception as e:
            logger.error(f"Markdown delivery failed: {e}")
    
    async def _deliver_to_telegram(self, digest: Digest, run_id: UUID):
        """æ¨é€åˆ° Telegram"""
        try:
            async with TelegramOutput() as output:
                success = await output.deliver(digest)
            
            if success:
                self.stats["delivered"] += 1
                logger.info("Delivered to Telegram")
            
        except Exception as e:
            logger.error(f"Telegram delivery failed: {e}")
    
    async def _deliver_to_email(self, digest: Digest, run_id: UUID):
        """å‘é€é‚®ä»¶"""
        try:
            async with EmailOutput() as output:
                success = await output.deliver(digest)
            
            if success:
                self.stats["delivered"] += 1
                logger.info("Delivered to Email")
            
        except Exception as e:
            logger.error(f"Email delivery failed: {e}")
    
    async def _generate_ticker_summaries(
        self,
        digest_items: List[DigestItem],
        watchlist: List[Dict],
        thesis_map: Dict[str, str]
    ) -> Dict[str, TickerSummary]:
        """ä¸ºæ¯åªè‚¡ç¥¨ç”Ÿæˆæ±‡æ€»åˆ†æ"""
        summaries: Dict[str, TickerSummary] = {}
        
        # æŒ‰ ticker åˆ†ç»„
        by_ticker: Dict[str, List[DigestItem]] = {}
        for item in digest_items:
            if item.news.tickers:
                for ticker in item.news.tickers:
                    if ticker not in by_ticker:
                        by_ticker[ticker] = []
                    by_ticker[ticker].append(item)
        
        if not by_ticker:
            return summaries
        
        # è·å–å…¬å¸åç§°æ˜ å°„
        company_names = {item["ticker"]: item.get("company_name", item["ticker"]) for item in watchlist}
        
        # ä½¿ç”¨ AI ç”Ÿæˆæ±‡æ€»
        try:
            provider = get_ai_provider()
        except Exception as e:
            logger.warning(f"No AI provider for ticker summaries: {e}")
            # æ—  AI æ—¶è¿”å›åŸºç¡€ç»Ÿè®¡
            for ticker, items in by_ticker.items():
                bullish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bullish")
                bearish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bearish")
                neutral = len(items) - bullish - bearish
                
                summaries[ticker] = TickerSummary(
                    ticker=ticker,
                    company_name=company_names.get(ticker, ticker),
                    news_count=len(items),
                    overall_sentiment="bullish" if bullish > bearish else "bearish" if bearish > bullish else "neutral",
                    summary=f"Today: {len(items)} news items ({bullish} bullish, {bearish} bearish)",
                    key_events=[i.news.title[:60] for i in items[:3]],
                    thesis_impact="Requires manual assessment",
                    action_suggestion="Continue monitoring",
                    risk_alerts=[],
                    bullish_count=bullish,
                    bearish_count=bearish,
                    neutral_count=neutral,
                )
            return summaries
        
        # æœ‰ AI æ—¶ç”Ÿæˆè¯¦ç»†æ±‡æ€»
        logger.info(f"Generating AI summaries for {len(by_ticker)} tickers...")
        
        async with provider:
            for ticker, items in by_ticker.items():
                try:
                    company_name = company_names.get(ticker, ticker)
                    thesis = thesis_map.get(ticker, "")
                    
                    # å‡†å¤‡æ–°é—»æ•°æ®
                    news_items = [(item.news, item.analysis) for item in items]
                    
                    # è°ƒç”¨ AI ç”Ÿæˆæ±‡æ€»
                    summary_data, tokens, cost = await provider.generate_ticker_summary(
                        ticker=ticker,
                        company_name=company_name,
                        news_items=news_items,
                        thesis=thesis
                    )
                    
                    # ç»Ÿè®¡æƒ…ç»ª
                    bullish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bullish")
                    bearish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bearish")
                    neutral = len(items) - bullish - bearish
                    
                    summaries[ticker] = TickerSummary(
                        ticker=ticker,
                        company_name=company_name,
                        news_count=len(items),
                        overall_sentiment=summary_data.get("overall_sentiment", "neutral"),
                        summary=summary_data.get("summary", ""),
                        key_events=summary_data.get("key_events", [])[:3],
                        thesis_impact=summary_data.get("thesis_impact", ""),
                        action_suggestion=summary_data.get("action_suggestion", "ç»§ç»­è§‚å¯Ÿ"),
                        risk_alerts=summary_data.get("risk_alerts", []),
                        bullish_count=bullish,
                        bearish_count=bearish,
                        neutral_count=neutral,
                    )
                    
                    logger.info(f"Generated summary for {ticker}: {summary_data.get('overall_sentiment')}")
                    
                except Exception as e:
                    logger.warning(f"Failed to generate summary for {ticker}: {e}")
                    # æ·»åŠ åŸºç¡€æ±‡æ€»
                    bullish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bullish")
                    bearish = sum(1 for i in items if i.analysis and i.analysis.impact_direction == "bearish")
                    
                    summaries[ticker] = TickerSummary(
                        ticker=ticker,
                        company_name=company_names.get(ticker, ticker),
                        news_count=len(items),
                        overall_sentiment="bullish" if bullish > bearish else "bearish" if bearish > bullish else "neutral",
                        summary=f"Today: {len(items)} news items ({bullish} bullish, {bearish} bearish)",
                        key_events=[i.news.title[:60] for i in items[:3]],
                        thesis_impact="Summary generation failed",
                        action_suggestion="Continue monitoring",
                        risk_alerts=[],
                        bullish_count=bullish,
                        bearish_count=bearish,
                        neutral_count=len(items) - bullish - bearish,
                    )
        
        return summaries
    
    async def _update_pipeline_run(
        self,
        run_id: UUID,
        status: str,
        error_log: Optional[str] = None
    ):
        """æ›´æ–° Pipeline Run çŠ¶æ€"""
        try:
            async with async_session_maker() as db:
                await crud.update_pipeline_run(db, run_id, PipelineRunUpdate(
                    finished_at=datetime.utcnow(),
                    status=status,
                    raw_collected=self.stats["raw_collected"],
                    after_normalize=self.stats["after_normalize"],
                    after_dedup=self.stats["after_dedup"],
                    analyzed_success=self.stats["analyzed_success"],
                    analyzed_failed=self.stats["analyzed_failed"],
                    delivered=self.stats["delivered"],
                    error_log=error_log,
                ))
                await db.commit()
        except Exception as e:
            logger.error(f"Failed to update pipeline run: {e}")


async def run_pipeline(
    run_id: Optional[UUID] = None,
    hours_lookback: int = 24,
    tickers: Optional[List[str]] = None,
    limit_per_ticker: Optional[int] = None
) -> Digest:
    """
    è¿è¡Œ Pipeline çš„ä¾¿æ·å‡½æ•°ï¼ˆç”¨äº API å’Œ CLIï¼‰
    """
    pipeline = Pipeline(hours_lookback=hours_lookback, tickers=tickers, limit_per_ticker=limit_per_ticker)
    return await pipeline.run(run_id)
